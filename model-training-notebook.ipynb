{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Deploy a Neural Collaborative Filtering Model\n",
    "\n",
    "In this notebook, you will execute code blocks to\n",
    "\n",
    "1. inspect the training script [ncf.py](./ncf.py)  \n",
    "2. train a model using [Tensorflow Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html)  \n",
    "3. deploy and host the trained model as an endpoint using Amazon SageMaker Hosting Services  \n",
    "4. perform batch inference by calling the model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the last notebook (data-preparation-notebook.ipynb), we stored two variables.\n",
    "# Let's restore those variables here. These variables are inputs for the model training process.\n",
    "\n",
    "%store -r n_user\n",
    "%store -r n_item\n",
    "\n",
    "print(n_user)\n",
    "print(n_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requirements\n",
    "import os\n",
    "import json\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# get current SageMaker session's execution role and default bucket name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"execution role ARN:\", role)\n",
    "\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "print(\"default bucket name:\", bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the location of the training data\n",
    "training_data_uri = os.path.join(f's3://{bucket_name}', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inspect the training script using `pygmentize` magic\n",
    "!pygmentize 'ncf.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# specify training instance type and model hyperparameters\n",
    "# note that for the demo purpose, the number of epoch is set to 1\n",
    "\n",
    "num_of_instance = 1                 # number of instance to use for training\n",
    "instance_type = 'ml.c5.2xlarge'     # type of instance to use for training\n",
    "\n",
    "training_script = 'ncf.py'\n",
    "\n",
    "training_parameters = {\n",
    "    'epochs': 1,\n",
    "    'batch_size': 256, \n",
    "    'n_user': n_user, \n",
    "    'n_item': n_item\n",
    "}\n",
    "\n",
    "# training framework specs\n",
    "tensorflow_version = '2.1.0'\n",
    "python_version = 'py3'\n",
    "distributed_training_spec = {'parameter_server': {'enabled': True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the training job using Tensorflow estimator\n",
    "ncf_estimator = TensorFlow(\n",
    "    entry_point=training_script,\n",
    "    role=role,\n",
    "    train_instance_count=num_of_instance,\n",
    "    train_instance_type=instance_type,\n",
    "    framework_version=tensorflow_version,\n",
    "    py_version=python_version,\n",
    "    distributions=distributed_training_spec,\n",
    "    hyperparameters=training_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kick off the training job\n",
    "ncf_estimator.fit(training_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once the model is trained, we can deploy the model using Amazon SageMaker Hosting Services\n",
    "# Here we deploy the model using one ml.c5.xlarge instance as a tensorflow-serving endpoint\n",
    "# This enables us to invoke the endpoint like how we use Tensorflow serving\n",
    "# Read more about Tensorflow serving using the link below\n",
    "# https://www.tensorflow.org/tfx/tutorials/serving/rest_simple\n",
    "\n",
    "endpoint_name = 'neural-collaborative-filtering-model-demo'\n",
    "\n",
    "predictor = ncf_estimator.deploy(initial_instance_count=1, \n",
    "                                 instance_type='ml.c5.xlarge', \n",
    "                                 endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the endpoint in another notebook, we can initiate a predictor object as follows\n",
    "from sagemaker.tensorflow import TensorFlowPredictor\n",
    "\n",
    "predictor = TensorFlowPredictor(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read testing data\n",
    "def _load_testing_data(base_dir):\n",
    "    \"\"\" load testing data \"\"\"\n",
    "    df_test = np.load(os.path.join(base_dir, 'test.npy'))\n",
    "    user_test, item_test, y_test = np.split(np.transpose(df_test).flatten(), 3)\n",
    "    return user_test, item_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read testing data from local\n",
    "user_test, item_test, test_labels = _load_testing_data('./ml-latest-small/s3/')\n",
    "\n",
    "# one-hot encode the testing data for model input\n",
    "with tf.compat.v1.Session() as tf_sess:\n",
    "    test_user_data = tf_sess.run(tf.one_hot(user_test, depth=n_user)).tolist()\n",
    "    test_item_data = tf_sess.run(tf.one_hot(item_test, depth=n_item)).tolist()\n",
    "    \n",
    "# if you're using Tensorflow 2.0 for one hot encoding\n",
    "# you can convert the tensor to list using:\n",
    "# tf.one_hot(uuser_test, depth=n_user).numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make batch prediction\n",
    "batch_size = 100\n",
    "y_pred = []\n",
    "for idx in range(0, len(test_user_data), batch_size):\n",
    "    # reformat test samples into tensorflow serving acceptable format\n",
    "    input_vals = {\n",
    "     \"instances\": [\n",
    "         {'input_1': u, 'input_2': i} \n",
    "         for (u, i) in zip(test_user_data[idx:idx+batch_size], test_item_data[idx:idx+batch_size])\n",
    "    ]}\n",
    " \n",
    "    # invoke model endpoint to make inference\n",
    "    pred = predictor.predict(input_vals)\n",
    "    \n",
    "    # store predictions\n",
    "    y_pred.extend([i[0] for i in pred['predictions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see some prediction examples, assuming the threshold \n",
    "# --- prediction probability view ---\n",
    "print('This is what the prediction output looks like')\n",
    "print(y_pred[:5], end='\\n\\n\\n')\n",
    "\n",
    "# --- user item pair prediction view, with threshold of 0.5 applied ---\n",
    "pred_df = pd.DataFrame([\n",
    "    user_test,\n",
    "    item_test,\n",
    "    (np.array(y_pred) >= 0.5).astype(int)],\n",
    ").T\n",
    "\n",
    "pred_df.columns = ['userId', 'movieId', 'prediction']\n",
    "\n",
    "print('We can convert the output to user-item pair as shown below')\n",
    "print(pred_df.head(), end='\\n\\n\\n')\n",
    "\n",
    "# --- aggregated prediction view, by user ---\n",
    "print('Lastly, we can roll up the prediction list by user and view it that way')\n",
    "print(pred_df.query('prediction == 1').groupby('userId').movieId.apply(list).head().to_frame(), end='\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint at the end of the demo\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
